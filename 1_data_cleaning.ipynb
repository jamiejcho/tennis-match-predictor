{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafd4557",
   "metadata": {},
   "source": [
    "## Notebook 1: Data Ingestion, Standardization, and Cleaning\n",
    "\n",
    "In this notebook, we will build the foundation for our Australian Open 2026 prediction model. Our raw data consists of individual CSV files for every year of professional tennis match results (from 1968 to 2026).\n",
    "\n",
    "Our goals are:\n",
    "1. Ingest: Load match data sequentially from 1968 through the current 2026 season.\n",
    "1. Concatenate: Merge these yearly files into a single, unified \"Master DataFrame.\"\n",
    "1. Clean: Handle missing values logically. We will specific tennis domain knowledge to fill gaps (e.g., Unseeded players are not \"Seed 0\", they are \"Seed 100+\").\n",
    "1. Standardize: Ensure date columns are actual datetime objects and categorical data is consistent.\n",
    "1. Export: Save the cleaned data as a single CSV file (master_data_cleaned.csv) for use in feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a67b2",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "We will rely on pandas for data manipulation and numpy for numerical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "6f0deb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set pandas display options to ensure we can see all columns during inspection\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98409b96",
   "metadata": {},
   "source": [
    "## 2.. Data Ingestion\n",
    "\n",
    "Instead of loading everything blindly, we will iterate through the years 1968 to 2026. This allows us to explicitly control the scope of our historical data.\n",
    "\n",
    "Our data is stored under the \"./data/\" directory with each file named \"YYYY.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "93804a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data ingestion from 1968 to 2026...\n",
      "Successfully loaded: 1970.csv (3236 matches)\n",
      "Successfully loaded: 1980.csv (3909 matches)\n",
      "Successfully loaded: 1990.csv (3683 matches)\n",
      "Successfully loaded: 2000.csv (3378 matches)\n",
      "Successfully loaded: 2010.csv (3030 matches)\n",
      "Successfully loaded: 2020.csv (1466 matches)\n",
      "Successfully loaded: 2026.csv (137 matches)\n",
      "--------------------------------------------------\n",
      "Ingestion complete.\n",
      "Total matches loaded: 198,063\n"
     ]
    }
   ],
   "source": [
    "# Define the path and the range of years we want to analyze\n",
    "data_path = './data/'\n",
    "start_year = 1968\n",
    "end_year = 2026  # Includes the current year's data up to the AO\n",
    "\n",
    "# Initialize an empty list to store each year's dataframe\n",
    "all_matches = []\n",
    "\n",
    "print(f\"Starting data ingestion from {start_year} to {end_year}...\")\n",
    "\n",
    "# Loop through each year\n",
    "for year in range(start_year, end_year + 1):\n",
    "    file_path = f\"{data_path}{year}.csv\"\n",
    "    \n",
    "    # Check if the file exists before trying to read it\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            # low_memory=False helps pandas guess data types accurately for large files\n",
    "            current_df = pd.read_csv(file_path, low_memory=False)\n",
    "            \n",
    "            # Append to our list\n",
    "            all_matches.append(current_df)\n",
    "            \n",
    "            # Print status every 10 years to avoid cluttering the output\n",
    "            if year % 10 == 0 or year == end_year:\n",
    "                print(f\"Successfully loaded: {year}.csv ({len(current_df)} matches)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {year}.csv: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: File not found for year {year}\")\n",
    "\n",
    "# Concatenate all yearly dataframes into one Master DataFrame\n",
    "df_master = pd.concat(all_matches, ignore_index=True)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Ingestion complete.\\nTotal matches loaded: {len(df_master):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045eec7",
   "metadata": {},
   "source": [
    "## 3. Date Parsing and Sorting\n",
    "\n",
    "Raw CSV data usually stores dates as integers (e.g., 20230115) or strings. For time-series forecasting, we must convert these to proper Python datetime objects. We also sort the data chronologically to ensure our future model doesn't \"peek\" at future matches during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5f4d6bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Range: 1968-01-01 00:00:00 to 2026-01-17 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Convert 'tourney_date' to datetime objects\n",
    "# errors='coerce' will turn any corrupted dates into NaT (Not a Time) so they don't crash the script\n",
    "df_master['tourney_date'] = pd.to_datetime(df_master['tourney_date'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "# Sort the data:\n",
    "# 1. By Date (Primary)\n",
    "# 2. By Tournament ID (Secondary - keeps tournament matches together)\n",
    "# 3. By Match Number (Tertiary - orders R128 -> R64 -> Final)\n",
    "df_master = df_master.sort_values(by=['tourney_date', 'tourney_id', 'match_num']).reset_index(drop=True)\n",
    "\n",
    "# Verify the range\n",
    "print(f\"Date Range: {df_master['tourney_date'].min()} to {df_master['tourney_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f105b",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Values (Data Imputation)\n",
    "Tennis data often has missing values (NaN). How we fill them matters significantly for model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34280e0f",
   "metadata": {},
   "source": [
    "### 4.1. Rankings\n",
    "Logic: In tennis, a Rank of 1 is the best. If we fill missing ranks with 0, the model will think that player is better than the World No. 1.\n",
    "\n",
    "Fix: We replace missing ranks with 9999 to signify \"Unranked\" or \"Low Rank\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fb634f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing rankings with 9999\n",
    "df_master['winner_rank'] = df_master['winner_rank'].fillna(9999).astype(int)\n",
    "df_master['loser_rank'] = df_master['loser_rank'].fillna(9999).astype(int)\n",
    "\n",
    "# Fill missing ranking points with 0 (Unranked players have 0 points)\n",
    "df_master['winner_rank_points'] = df_master['winner_rank_points'].fillna(0).astype(int)\n",
    "df_master['loser_rank_points'] = df_master['loser_rank_points'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9ad1a",
   "metadata": {},
   "source": [
    "### 4.2. Seeds & entries\n",
    "Logic for seeds: An unseeded player is effectively a very low seed. We use 100 to represent this. \n",
    "\n",
    "Logic for entries: Missing entry type usually means \"Standard\" (Direct Acceptance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "9546856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing seeds with 100\n",
    "df_master['winner_seed'] = df_master['winner_seed'].fillna(100).astype(int)\n",
    "df_master['loser_seed'] = df_master['loser_seed'].fillna(100).astype(int)\n",
    "\n",
    "# Fill missing entries with 'Std'\n",
    "df_master['winner_entry'] = df_master['winner_entry'].fillna('Std')\n",
    "df_master['loser_entry'] = df_master['loser_entry'].fillna('Std')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14948903",
   "metadata": {},
   "source": [
    "### 4.3. Physical attributes (height & age)\n",
    "Logic: A height of 0 cm or an age of 0 is impossible. \n",
    "\n",
    "Fix: We fill missing heights and ages with the median of the dataset. This is robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "6c5d9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate medians based on the entire dataset\n",
    "# We use nanmedian to ignore existing NaNs during calculation\n",
    "avg_winner_ht = df_master['winner_ht'].median()\n",
    "avg_loser_ht = df_master['loser_ht'].median()\n",
    "avg_winner_age = df_master['winner_age'].median()\n",
    "avg_loser_age = df_master['loser_age'].median()\n",
    "\n",
    "# Apply imputation\n",
    "df_master['winner_ht'] = df_master['winner_ht'].fillna(avg_winner_ht)\n",
    "df_master['loser_ht'] = df_master['loser_ht'].fillna(avg_loser_ht)\n",
    "df_master['winner_age'] = df_master['winner_age'].fillna(avg_winner_age)\n",
    "df_master['loser_age'] = df_master['loser_age'].fillna(avg_loser_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f237c43",
   "metadata": {},
   "source": [
    "### 4.4. Match statistics\n",
    "Logic: If match stats (z.B., aces, double faults, break points) are missing, it usually means the match statistics weren't recorded for that tournament, which are common in Futures/Challengers level.\n",
    "\n",
    "Fix: We assume 0 for these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4005a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of technical match statistics columns\n",
    "stat_cols = [\n",
    "    'minutes', \n",
    "    'w_ace', 'w_df', 'w_svpt', 'w_1stIn', 'w_1stWon', 'w_2ndWon', 'w_SvGms', 'w_bpSaved', 'w_bpFaced',\n",
    "    'l_ace', 'l_df', 'l_svpt', 'l_1stIn', 'l_1stWon', 'l_2ndWon', 'l_SvGms', 'l_bpSaved', 'l_bpFaced'\n",
    "]\n",
    "\n",
    "# Fill missing stats with 0\n",
    "for col in stat_cols:\n",
    "    if col in df_master.columns:\n",
    "        df_master[col] = df_master[col].fillna(0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749fbfd",
   "metadata": {},
   "source": [
    "### 4.5. Draw Size\n",
    "Logic: We cannot just use the median (32) for everything because a Grand Slam (128) is much harder to win than a Challenger (32). \n",
    "\n",
    "Fix: We map the draw_size based on the tourney_level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d9e82495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map draw size to Tournament Level\n",
    "draw_size_map = {\n",
    "    'G': 128,  # Grand Slam\n",
    "    'M': 64,   # Masters 1000 (standardized to 64, though some are 56/96)\n",
    "    'A': 32,   # ATP Tour (general)\n",
    "    '250': 32, # Specific level label\n",
    "    '500': 32, # Specific level label\n",
    "    'C': 32,   # Challengers\n",
    "    'S': 32,   # Satellites/Futures\n",
    "    'U': 18,   # United Cup\n",
    "    'F': 8,    # ATP Finals (year-end)\n",
    "    'D': 4,    # Davis Cup\n",
    "}\n",
    "\n",
    "# Fill based on the map\n",
    "df_master['draw_size'] = df_master['draw_size'].fillna(df_master['tourney_level'].map(draw_size_map))\n",
    "\n",
    "# Fallback: If level was unknown, use 32 (standard ATP size)\n",
    "df_master['draw_size'] = df_master['draw_size'].fillna(32).astype(int)\n",
    "\n",
    "# Fix missing match numbers (default to 1)\n",
    "df_master['match_num'] = df_master['match_num'].fillna(1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe496d",
   "metadata": {},
   "source": [
    "### 4.6. Other categorical Data\n",
    "Logic: We cannot mathematically impute a player's country or dominant hand or whether the court is indoor vs outdoor.\n",
    "\n",
    "Fix: We create a new category called \"Unknown\". This allows the random forest to see \"Unknown\" as a distinct group rather than crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ab0887f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['surface', 'indoor', 'winner_hand', 'loser_hand', 'winner_ioc', 'loser_ioc']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_master[col] = df_master[col].fillna('Unknown')\n",
    "\n",
    "# Standardize hand: Sometimes data has 'Right', 'R', or 'nan'. \n",
    "# We map them to 'R', 'L', and 'U', respectively.\n",
    "normalization_map = {'Right': 'R', 'Left': 'L', 'nan': 'U', 'Unknown': 'U'}\n",
    "df_master['winner_hand'] = df_master['winner_hand'].replace(normalization_map)\n",
    "df_master['loser_hand'] = df_master['loser_hand'].replace(normalization_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000a424",
   "metadata": {},
   "source": [
    "## 5. Final Cleanup and Export\n",
    "We drop the small number of rows where the score or loser_id is missing. These represent corrupted match records (e.g., walkovers without data) that cannot be used for training.\n",
    "\n",
    "Finally, we verify that our data is clean and save it as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "e78599f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining Missing Values (Top 10 columns):\n",
      "score         7\n",
      "loser_id      1\n",
      "tourney_id    0\n",
      "w_2ndWon      0\n",
      "best_of       0\n",
      "dtype: int64\n",
      "\n",
      "Remaining Missing Values: 0\n",
      "\n",
      "SUCCESS: Clean dataset saved to master_data_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Check if any nulls remain\n",
    "print(\"\\nRemaining Missing Values (Top 10 columns):\")\n",
    "print(df_master.isnull().sum().sort_values(ascending=False).head(5))\n",
    "\n",
    "# Drop corrupted rows (score or player IDs missing)\n",
    "df_master = df_master.dropna(subset=['score', 'winner_id', 'loser_id'])\n",
    "\n",
    "# Finally verify if any nulls remain\n",
    "missing_sum = df_master.isnull().sum().sum()\n",
    "print(f\"\\nRemaining Missing Values: {missing_sum}\")\n",
    "\n",
    "# Define output filename\n",
    "output_file = 'master_data_cleaned.csv'\n",
    "\n",
    "# Save the clean file\n",
    "# Set index=False to avoid creating an unnamed index column\n",
    "# When we load this CSV in the next notebook, we need to re-parse the date column because CSVs store dates as strings.\n",
    "df_master.to_csv(output_file, index=False)\n",
    "print(f\"\\nSUCCESS: Clean dataset saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
